{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Лабораторная работа 6.1. Transfer learning сверточных нейронных сетей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Работу выполнил:<span style=\"color:blue\"> Михаил Батухтин</span>"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сделанную лабораторную работу отправляйте через [ФОРМУ](https://vyatsu-my.sharepoint.com/:f:/g/personal/usr09019_vyatsu_ru/EpSy8Xu4FxhMklHgj0NBX-4B1VnXTBIs4-D3Oi74j0wcGQ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Необходимо дообучить свёрточню сеть ResNet50 для решения задачи классификации изображений из предыдущей лабораторной работы.\n",
    "Ниже приводится пример дообучения нейронной сети в PyTorch, взятый по [ссылке](https://github.com/spmallick/learnopencv/tree/master/Image-Classification-in-PyTorch).\n",
    "\n",
    "Задание считается выполненным при достижении accuracy=0.9 на тестовом наборе данных."
   ]
  },
  {
   "metadata": {
    "id": "SrL47MttOLS3"
   },
   "cell_type": "code",
   "source": [
    "import torch, torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "from PIL import Image"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "image_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(size=256, scale=(0.8, 1.0)),\n",
    "        transforms.RandomRotation(degrees=15),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.CenterCrop(size=224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                             [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'valid': transforms.Compose([\n",
    "        transforms.Resize(size=256),\n",
    "        transforms.CenterCrop(size=224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                             [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Resize(size=256),\n",
    "        transforms.CenterCrop(size=224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                             [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "}"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "dataset = '../data'\n",
    "\n",
    "train_directory = os.path.join(dataset, 'seg_train/seg_train')\n",
    "valid_directory = os.path.join(dataset, 'seg_test/seg_test')\n",
    "test_directory = os.path.join(dataset, 'seg_test/seg_test')\n",
    "\n",
    "bs = 32\n",
    "\n",
    "# Number of classes\n",
    "num_classes = len(os.listdir(train_directory))\n",
    "print(f\"Number of classes: {num_classes}\")\n",
    "\n",
    "# Load Data from folders\n",
    "data = {\n",
    "    'train': datasets.ImageFolder(root=train_directory, transform=image_transforms['train']),\n",
    "    'valid': datasets.ImageFolder(root=valid_directory, transform=image_transforms['valid']),\n",
    "    'test': datasets.ImageFolder(root=test_directory, transform=image_transforms['test'])\n",
    "}\n",
    "\n",
    "# Get a mapping of the indices to the class names\n",
    "idx_to_class = {v: k for k, v in data['train'].class_to_idx.items()}\n",
    "print(\"Class mapping:\", idx_to_class)\n",
    "\n",
    "# Size of Data\n",
    "train_data_size = len(data['train'])\n",
    "valid_data_size = len(data['valid'])\n",
    "test_data_size = len(data['test'])\n",
    "\n",
    "# Create iterators for the Data loaded using DataLoader module\n",
    "train_data_loader = DataLoader(data['train'], batch_size=bs, shuffle=True)\n",
    "valid_data_loader = DataLoader(data['valid'], batch_size=bs, shuffle=True)\n",
    "test_data_loader = DataLoader(data['test'], batch_size=bs, shuffle=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"Dataset sizes - Train: {train_data_size}, Valid: {valid_data_size}, Test: {test_data_size}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "resnet50 = models.resnet50(pretrained=True)\n",
    "resnet50 = resnet50.to(device)\n",
    "\n",
    "# Freeze model parameters\n",
    "for param in resnet50.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Change the final layer of ResNet50 Model for Transfer Learning\n",
    "fc_inputs = resnet50.fc.in_features\n",
    "\n",
    "resnet50.fc = nn.Sequential(\n",
    "    nn.Linear(fc_inputs, 256),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.4),\n",
    "    nn.Linear(256, num_classes),\n",
    "    nn.LogSoftmax(dim=1)\n",
    ")\n",
    "\n",
    "# Convert model to be used on GPU\n",
    "resnet50 = resnet50.to(device)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "loss_func = nn.NLLLoss()\n",
    "optimizer = optim.Adam(resnet50.parameters(), lr=0.001)\n",
    "\n",
    "def train_and_validate(model, loss_criterion, optimizer, epochs=25):\n",
    "    start = time.time()\n",
    "    history = []\n",
    "    best_loss = 100000.0\n",
    "    best_epoch = None\n",
    "    best_model_state = None\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        epoch_start = time.time()\n",
    "        print(\"Epoch: {}/{}\".format(epoch+1, epochs))\n",
    "\n",
    "        # Set to training mode\n",
    "        model.train()\n",
    "\n",
    "        # Loss and Accuracy within the epoch\n",
    "        train_loss = 0.0\n",
    "        train_acc = 0.0\n",
    "\n",
    "        valid_loss = 0.0\n",
    "        valid_acc = 0.0\n",
    "\n",
    "        for i, (inputs, labels) in enumerate(train_data_loader):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Clean existing gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = loss_criterion(outputs, labels)\n",
    "\n",
    "            # Backpropagate\n",
    "            loss.backward()\n",
    "\n",
    "            # Update parameters\n",
    "            optimizer.step()\n",
    "\n",
    "            # Compute the total loss for the batch and add it to train_loss\n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "            # Compute the accuracy\n",
    "            ret, predictions = torch.max(outputs.data, 1)\n",
    "            correct_counts = predictions.eq(labels.data.view_as(predictions))\n",
    "\n",
    "            # Convert correct_counts to float and compute mean\n",
    "            acc = torch.mean(correct_counts.type(torch.FloatTensor))\n",
    "\n",
    "            # Compute total accuracy in the whole batch\n",
    "            train_acc += acc.item() * inputs.size(0)\n",
    "\n",
    "        # Validation\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "\n",
    "            for j, (inputs, labels) in enumerate(valid_data_loader):\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                outputs = model(inputs)\n",
    "                loss = loss_criterion(outputs, labels)\n",
    "\n",
    "                valid_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "                ret, predictions = torch.max(outputs.data, 1)\n",
    "                correct_counts = predictions.eq(labels.data.view_as(predictions))\n",
    "                acc = torch.mean(correct_counts.type(torch.FloatTensor))\n",
    "                valid_acc += acc.item() * inputs.size(0)\n",
    "\n",
    "        if valid_loss < best_loss:\n",
    "            best_loss = valid_loss\n",
    "            best_epoch = epoch\n",
    "            best_model_state = model.state_dict().copy()\n",
    "\n",
    "        # Find average losses and accuracy\n",
    "        avg_train_loss = train_loss/train_data_size\n",
    "        avg_train_acc = train_acc/train_data_size\n",
    "        avg_valid_loss = valid_loss/valid_data_size\n",
    "        avg_valid_acc = valid_acc/valid_data_size\n",
    "\n",
    "        history.append([avg_train_loss, avg_valid_loss, avg_train_acc, avg_valid_acc])\n",
    "\n",
    "        epoch_end = time.time()\n",
    "\n",
    "        print(\"Epoch : {:03d}, Training: Loss: {:.4f}, Accuracy: {:.4f}%, \\n\\t\\tValidation : Loss: {:.4f}, Accuracy: {:.4f}%, Time: {:.4f}s\".format(\n",
    "            epoch, avg_train_loss, avg_train_acc*100, avg_valid_loss, avg_valid_acc*100, epoch_end-epoch_start))\n",
    "\n",
    "        # Save model checkpoint\n",
    "        torch.save(model.state_dict(), f\"{dataset}_model_{epoch}.pth\")\n",
    "\n",
    "    # Load best model state\n",
    "    model.load_state_dict(best_model_state)\n",
    "    return model, history, best_epoch\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "num_epochs = 20\n",
    "trained_model, history, best_epoch = train_and_validate(resnet50, loss_func, optimizer, num_epochs)\n",
    "\n",
    "# Save final model and history\n",
    "torch.save(trained_model.state_dict(), f\"{dataset}_final_model.pth\")\n",
    "torch.save(history, f\"{dataset}_history.pt\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "history = np.array(history)\n",
    "plt.plot(history[:,0:2])\n",
    "plt.legend(['Tr Loss', 'Val Loss'])\n",
    "plt.xlabel('Epoch Number')\n",
    "plt.ylabel('Loss')\n",
    "plt.ylim(0,1)\n",
    "plt.savefig(f'{dataset}_loss_curve.png')\n",
    "plt.show()\n",
    "\n",
    "# Plot accuracy curve\n",
    "plt.plot(history[:,2:4])\n",
    "plt.legend(['Tr Accuracy', 'Val Accuracy'])\n",
    "plt.xlabel('Epoch Number')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim(0,1)\n",
    "plt.savefig(f'{dataset}_accuracy_curve.png')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def computeTestSetAccuracy(model, loss_criterion):\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.eval()\n",
    "\n",
    "    test_acc = 0.0\n",
    "    test_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for j, (inputs, labels) in enumerate(test_data_loader):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_criterion(outputs, labels)\n",
    "\n",
    "            test_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "            ret, predictions = torch.max(outputs.data, 1)\n",
    "            correct_counts = predictions.eq(labels.data.view_as(predictions))\n",
    "            acc = torch.mean(correct_counts.type(torch.FloatTensor))\n",
    "            test_acc += acc.item() * inputs.size(0)\n",
    "\n",
    "    avg_test_loss = test_loss/test_data_size\n",
    "    avg_test_acc = test_acc/test_data_size\n",
    "\n",
    "    print(f\"Test Loss: {avg_test_loss:.4f}, Test Accuracy: {avg_test_acc:.4f}\")\n",
    "\n",
    "def predict(model, test_image_name):\n",
    "    transform = image_transforms['test']\n",
    "\n",
    "    test_image = Image.open(test_image_name)\n",
    "    plt.imshow(test_image)\n",
    "    plt.show()\n",
    "\n",
    "    test_image_tensor = transform(test_image)\n",
    "    if torch.cuda.is_available():\n",
    "        test_image_tensor = test_image_tensor.view(1, 3, 224, 224).cuda()\n",
    "    else:\n",
    "        test_image_tensor = test_image_tensor.view(1, 3, 224, 224)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        out = model(test_image_tensor)\n",
    "        ps = torch.exp(out)\n",
    "        topk, topclass = ps.topk(3, dim=1)\n",
    "\n",
    "        for i in range(3):\n",
    "            print(\"Prediction\", i+1, \":\", idx_to_class[topclass.cpu().numpy()[0][i]],\n",
    "                  \", Score: \", topk.cpu().numpy()[0][i])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model = trained_model\n",
    "computeTestSetAccuracy(model, loss_func)"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "image_classification_using_transfer_learning_in_pytorch.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "39dc1b0906487b45ba344ffeedd791fdcc86df127220ba63f0709de5586920fd"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "name": "python3",
   "language": "python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
