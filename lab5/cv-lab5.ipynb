{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-9xebZYy6D4f"
   },
   "source": [
    "# Лабораторная работа 5. Классификация изображений с помощью сверточной нейронной сети"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PLCg87Ae6D4l"
   },
   "source": [
    "### Работу выполнил:<span style=\"color:blue\"> {ваше имя и фамилия}</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yaN2g25d6D4m"
   },
   "source": [
    "### Сделанную лабораторную работу отправляйте через [ФОРМУ](https://vyatsu-my.sharepoint.com/:f:/g/personal/usr09019_vyatsu_ru/Ei2kQNkh2ABHuYFR4BPtyrEBRGDgM28MZCQVhyrDQOEOGA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j_Fi5dPS6D4o"
   },
   "source": [
    "## Задание 1.\n",
    "\n",
    "Построить сверточную нейронную сеть для классификации изображений на основе датасета Intel Image Classification (https://www.kaggle.com/datasets/puneet6060/intel-image-classification). Задание предполагает самостоятельное создание структуры сети и обучение её с нуля.\n",
    "\n",
    "В работе можно ориентироваться на структуру сети [AlexNet](https://en.wikipedia.org/wiki/AlexNet).\n",
    "\n",
    "Ниже приводится пример создания сети с использованием библиотеки PyTorch. Данный пример не является готовым решением: сеть переобучилась и на тестовых данных показывает низкое значение метрик качества (accuracy/loss).\n",
    "\n",
    "Задание считается выполненным при достижении accuracy=0.83 на тестовом наборе данных."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "tug-QzrV6D4p"
   },
   "source": [
    "# Импортируем необходимые модули\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "import matplotlib.pyplot as plt"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "XAEBmTKZ6D4q"
   },
   "source": [
    "# Директории обучающих и тестовых данных\n",
    "data_dir = \"../data/seg_train/seg_train\"\n",
    "test_data_dir = \"../data/seg_test/seg_test\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "cVU-pqz26D4r"
   },
   "source": [
    "# Улучшенные преобразования с аугментацией для обучающих данных\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((150, 150)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),  # Случайное отражение по горизонтали\n",
    "    transforms.RandomRotation(degrees=15),   # Случайный поворот на ±15 градусов\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),  # Изменение цветовых характеристик\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Преобразования для валидационных и тестовых данных (без аугментации)\n",
    "val_test_transform = transforms.Compose([\n",
    "    transforms.Resize((150, 150)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Загружаем данные с разными преобразованиями\n",
    "dataset = ImageFolder(data_dir, transform=train_transform)\n",
    "test_dataset = ImageFolder(test_data_dir, transform=val_test_transform)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "2etht-Wc6D4s",
    "outputId": "37296c80-da26-476f-8801-764e471a61d0"
   },
   "source": [
    "# Посмотрим на размерность данных и метки\n",
    "img, label = dataset[0]\n",
    "print(img.shape, label)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "FT4NXrfy6D4t",
    "outputId": "dcbe1989-469f-4e63-f631-fa4c694a2d36"
   },
   "source": [
    "print(\"Классы изображений : \\n\", dataset.classes)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "wflsgapt6D4v",
    "outputId": "54ec86ef-7426-4bcc-f109-927473daec1c"
   },
   "source": [
    "# Определим функцию для отображения изображений датасета\n",
    "def display_img(img, label):\n",
    "    print(f\"Label : {dataset.classes[label]}\")\n",
    "    plt.imshow(img.permute(1,2,0))  # Преобразуем (3, 150, 150) в (150, 150, 3)\n",
    "\n",
    "# Отобразим первое изображение датасета\n",
    "display_img(*dataset[0])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Ru4tQo3f6D4w"
   },
   "source": [
    "# Определяем структуру нейронной сети с Batch Normalization и Dropout\n",
    "import torch.nn as nn\n",
    "\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, numChannels, classes):\n",
    "        # Вызываем родительский конструктор\n",
    "        super(ConvNet, self).__init__()\n",
    "\n",
    "        # Первый набор слоев CONV => BN => RELU => POOL\n",
    "        self.conv1 = nn.Conv2d(in_channels=numChannels, out_channels=32, kernel_size=(3, 3), padding=(1, 1))\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2))\n",
    "\n",
    "        # Второй набор слоев CONV => BN => RELU => POOL\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=(3, 3), padding=(1, 1))\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2))\n",
    "\n",
    "        # Третий набор слоев CONV => BN => RELU => POOL\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=(3, 3), padding=(1, 1))\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.maxpool3 = nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2))\n",
    "        self.dropout3 = nn.Dropout(0.25)  # Dropout после третьего сверточного блока\n",
    "\n",
    "        # Первый набор слоев FC => BN => RELU => DROPOUT\n",
    "        self.fc1 = nn.Linear(in_features=41472, out_features=1024)\n",
    "        self.bn4 = nn.BatchNorm1d(1024)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.dropout4 = nn.Dropout(0.5)  # Более высокий dropout для полносвязных слоев\n",
    "\n",
    "        # Второй набор слоев FC => BN => RELU => DROPOUT\n",
    "        self.fc2 = nn.Linear(in_features=1024, out_features=512)\n",
    "        self.bn5 = nn.BatchNorm1d(512)\n",
    "        self.relu5 = nn.ReLU()\n",
    "\n",
    "        # Инициализируем классификатор softmax\n",
    "        self.fc3 = nn.Linear(in_features=512, out_features=classes)\n",
    "        self.logSoftmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Первый блок: CONV => BN => RELU => POOL => DROPOUT\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.maxpool1(x)\n",
    "\n",
    "        # Второй блок: CONV => BN => RELU => POOL => DROPOUT\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.maxpool2(x)\n",
    "\n",
    "        # Третий блок: CONV => BN => RELU => POOL => DROPOUT\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.maxpool3(x)\n",
    "        x = self.dropout3(x)\n",
    "\n",
    "        # Переводим результат предыдущего слоя в одномерный вид\n",
    "        x = torch.flatten(x, 1)\n",
    "\n",
    "        # Полносвязные слои с BatchNorm и Dropout\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn4(x)\n",
    "        x = self.relu4(x)\n",
    "        x = self.dropout4(x)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        x = self.bn5(x)\n",
    "        x = self.relu5(x)\n",
    "\n",
    "        # Классификатор (без dropout)\n",
    "        x = self.fc3(x)\n",
    "        output = self.logSoftmax(x)\n",
    "\n",
    "        return output"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "g1oBjzh16D4x",
    "outputId": "9d6fbbed-d177-43af-8fb2-9c95dece2154"
   },
   "source": [
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "# Определяем гиперпараметры\n",
    "BATCH_SIZE = 32\n",
    "INIT_LR = 1e-3\n",
    "EPOCHS = 15\n",
    "TRAIN_SPLIT = 0.75\n",
    "VAL_SPLIT = 1 - TRAIN_SPLIT\n",
    "\n",
    "train_size = int(len(dataset) * TRAIN_SPLIT)\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "# Определяем устройство, которое будет использоваться для обучения модели\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "train_data, val_data = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "print(f\"Размер обучающих данных: {len(train_data)}\")\n",
    "print(f\"Размер валидационных данных : {len(val_data)}\")\n",
    "\n",
    "# Загружаем обучающие, валидационные и тестовые данные в батчи\n",
    "train_dl = DataLoader(train_data, BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "val_dl = DataLoader(val_data, BATCH_SIZE, num_workers=0)\n",
    "test_dl = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "# Вычисляем количество шагов на одну эпоху для обучающего и валидационного набора\n",
    "train_steps = len(train_dl.dataset) // BATCH_SIZE\n",
    "val_steps = len(val_dl.dataset) // BATCH_SIZE"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ZLYRIXIL6D4y"
   },
   "source": [
    "from torch.optim import Adam\n",
    "import time\n",
    "\n",
    "# Инициализируем модель нейронной сети\n",
    "model = ConvNet(\n",
    "\tnumChannels=3,\n",
    "\tclasses=len(dataset.classes)).to(device)\n",
    "# Инициализируем оптимизатор и функцию потерь\n",
    "opt = Adam(model.parameters(), lr=INIT_LR, weight_decay=1e-4)\n",
    "loss_fn = nn.NLLLoss()\n",
    "# Инициализируем словарь для сохранения истории обучения\n",
    "H = {\n",
    "\t\"train_loss\": [],\n",
    "\t\"train_acc\": [],\n",
    "\t\"val_loss\": [],\n",
    "\t\"val_acc\": []\n",
    "}"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "yA1AtpdM6D4z",
    "outputId": "0acd1874-e70a-4033-deae-a0acb13ce233"
   },
   "source": [
    "# Будем замерять, как долго длился процесс обучения\n",
    "start_time = time.time()\n",
    "\n",
    "# Цикл по эпохам\n",
    "for e in range(0, EPOCHS):\n",
    "\t# Переводим модель в режим обучения\n",
    "\tmodel.train()\n",
    "\t# Переменные для хранения общих потерь обучения и валидации\n",
    "\ttotal_train_loss = 0\n",
    "\ttotal_val_loss = 0\n",
    "\t# Количество корректных предсказаний на шаге обучения\n",
    "\t# и валидации\n",
    "\ttrain_сorrect = 0\n",
    "\tval_сorrect = 0\n",
    "\t# Цикл по обучающему множеству\n",
    "\tfor (x, y) in train_dl:\n",
    "\t\t# Посылаем данные на устройство\n",
    "\t\t(x, y) = (x.to(device), y.to(device))\n",
    "\t\t# Выполняем прямой проход и считаем потери при обучении\n",
    "\t\tpred = model(x)\n",
    "\t\tloss = loss_fn(pred, y)\n",
    "\t\t# Обнуляем градиенты, выполняем шаг обратного распространения ошибки,\n",
    "\t\t# и обновляем веса\n",
    "\t\topt.zero_grad()\n",
    "\t\tloss.backward()\n",
    "\t\topt.step()\n",
    "\t\t# Добавляем потери к общим потерям и\n",
    "\t\t# вычисляем количество правильных предсказаний\n",
    "\t\ttotal_train_loss += loss\n",
    "\t\ttrain_сorrect += (pred.argmax(1) == y).type(\n",
    "\t\t\ttorch.float).sum().item()\n",
    "\n",
    "\t# Отключаем автоградиент для определения качества\n",
    "\twith torch.no_grad():\n",
    "\t\t# Переводим модель в режим определения качества\n",
    "\t\tmodel.eval()\n",
    "\t\t# Цикл по валидационному множеству\n",
    "\t\tfor (x, y) in val_dl:\n",
    "\t\t\t# Посылаем данные на устройство\n",
    "\t\t\t(x, y) = (x.to(device), y.to(device))\n",
    "\t\t\t# Выполняем предсказания и находим потери при валидации\n",
    "\t\t\tpred = model(x)\n",
    "\t\t\ttotal_val_loss += loss_fn(pred, y)\n",
    "\t\t\t# Вычисляем количество правильных предсказаний\n",
    "\t\t\tval_сorrect += (pred.argmax(1) == y).type(\n",
    "\t\t\t\ttorch.float).sum().item()\n",
    "\n",
    "\t# Вычисляем средние потери при обучении и валидации\n",
    "\tavg_train_loss = total_train_loss / train_steps\n",
    "\tavg_val_loss = total_val_loss / val_steps\n",
    "\t# Вычисляем accuracy при обучении и валидации\n",
    "\ttrain_сorrect = train_сorrect / len(train_dl.dataset)\n",
    "\tval_сorrect = val_сorrect / len(val_dl.dataset)\n",
    "\t# Обновляем историю обучения\n",
    "\tH[\"train_loss\"].append(avg_train_loss.cpu().detach().numpy())\n",
    "\tH[\"train_acc\"].append(train_сorrect)\n",
    "\tH[\"val_loss\"].append(avg_val_loss.cpu().detach().numpy())\n",
    "\tH[\"val_acc\"].append(val_сorrect)\n",
    "\t# Выводим информации оо обучении и валидации модели\n",
    "\tprint(\"[INFO] EPOCH: {}/{}\".format(e + 1, EPOCHS))\n",
    "\tprint(\"Train loss: {:.6f}, Train accuracy: {:.4f}\".format(\n",
    "\t\tavg_train_loss, train_сorrect))\n",
    "\tprint(\"Val loss: {:.6f}, Val accuracy: {:.4f}\\n\".format(\n",
    "\t\tavg_val_loss, val_сorrect))\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"[INFO] total time taken to train the model: {:.2f}s\".format(\n",
    "\tend_time - start_time))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "MH5HvXiO6D40",
    "outputId": "2f442680-f432-4439-8e96-c1e6dc63c555"
   },
   "source": [
    "# Сейчас мы можем оценить качество модели на тестовом наборе данных\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "\n",
    "# Отключаем автоградиент для определения качества\n",
    "with torch.no_grad():\n",
    "\t# Переводим модель в режим определения качества\n",
    "\tmodel.eval()\n",
    "\n",
    "\t# Список для хранения предсказаний\n",
    "\tpreds = []\n",
    "\t# Цикл по тестовому набору данных\n",
    "\tfor (x, y) in test_dl:\n",
    "\t\t# Посылаем данные на устройство\n",
    "\t\tx = x.to(device)\n",
    "\t\t# Делаем предсказание и помещаем в список\n",
    "\t\tpred = model(x)\n",
    "\t\tpreds.extend(pred.argmax(axis=1).cpu().numpy())\n",
    "# Генерируем отчёт о классификации\n",
    "print(classification_report(test_dataset.targets, np.array(preds), target_names=test_dataset.classes))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "MfoJ9vd56D41",
    "outputId": "b6df7261-ef1e-4fd2-8fc9-5b8265a400f4"
   },
   "source": [
    "# График потерь и accuracy при обучении\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "plt.plot(H[\"train_loss\"], label=\"train_loss\")\n",
    "plt.plot(H[\"val_loss\"], label=\"val_loss\")\n",
    "plt.plot(H[\"train_acc\"], label=\"train_acc\")\n",
    "plt.plot(H[\"val_acc\"], label=\"val_acc\")\n",
    "plt.title(\"Training Loss and Accuracy on Dataset\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend(loc=\"lower left\")\n",
    "\n",
    "# Сохраняем модель на диск\n",
    "torch.save(model, \"cnn.pt\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q_6BSDoh6D42"
   },
   "source": [
    "## Задание 2.\n",
    "\n",
    "Написать функцию, выполняющую предсказание класса для входного изображения. На вход функция принимает имя файла, а возвращает предсказанный класс."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "OuEOFyrO6D42",
    "ExecuteTime": {
     "end_time": "2025-10-02T17:20:43.322597Z",
     "start_time": "2025-10-02T17:20:43.222620Z"
    }
   },
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "def predict_image_class(model_path, image_path, class_names):\n",
    "\n",
    "    model = torch.load(model_path, warn=True)\n",
    "    model.eval()\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((150, 150)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "    # Загружаем и преобразуем изображение\n",
    "    image = Image.open(image_path)\n",
    "    image = transform(image).unsqueeze(0)  # Добавляем размерность батча\n",
    "\n",
    "    # Предсказание\n",
    "    with torch.no_grad():\n",
    "        output = model(image)\n",
    "        predicted_class = torch.argmax(output, 1).item()\n",
    "\n",
    "    return class_names[predicted_class]\n",
    "\n",
    "# Пример использования\n",
    "class_names = ['buildings', 'forest', 'glacier', 'mountain', 'sea', 'street']\n",
    "predicted_class = predict_image_class(\"cnn.pt\", \"example_image.jpg\", class_names)\n",
    "print(f\"Предсказанный класс: {predicted_class}\")"
   ],
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Unpickler.__init__() got an unexpected keyword argument 'warn'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mTypeError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[50]\u001B[39m\u001B[32m, line 27\u001B[39m\n\u001B[32m     25\u001B[39m \u001B[38;5;66;03m# Пример использования\u001B[39;00m\n\u001B[32m     26\u001B[39m class_names = [\u001B[33m'\u001B[39m\u001B[33mbuildings\u001B[39m\u001B[33m'\u001B[39m, \u001B[33m'\u001B[39m\u001B[33mforest\u001B[39m\u001B[33m'\u001B[39m, \u001B[33m'\u001B[39m\u001B[33mglacier\u001B[39m\u001B[33m'\u001B[39m, \u001B[33m'\u001B[39m\u001B[33mmountain\u001B[39m\u001B[33m'\u001B[39m, \u001B[33m'\u001B[39m\u001B[33msea\u001B[39m\u001B[33m'\u001B[39m, \u001B[33m'\u001B[39m\u001B[33mstreet\u001B[39m\u001B[33m'\u001B[39m]\n\u001B[32m---> \u001B[39m\u001B[32m27\u001B[39m predicted_class = \u001B[43mpredict_image_class\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mcnn.pt\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mexample_image.jpg\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mclass_names\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     28\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mПредсказанный класс: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpredicted_class\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[50]\u001B[39m\u001B[32m, line 6\u001B[39m, in \u001B[36mpredict_image_class\u001B[39m\u001B[34m(model_path, image_path, class_names)\u001B[39m\n\u001B[32m      4\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mpredict_image_class\u001B[39m(model_path, image_path, class_names):\n\u001B[32m----> \u001B[39m\u001B[32m6\u001B[39m     model = \u001B[43mtorch\u001B[49m\u001B[43m.\u001B[49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mwarn\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[32m      7\u001B[39m     model.eval()\n\u001B[32m      9\u001B[39m     transform = transforms.Compose([\n\u001B[32m     10\u001B[39m         transforms.Resize((\u001B[32m150\u001B[39m, \u001B[32m150\u001B[39m)),\n\u001B[32m     11\u001B[39m         transforms.ToTensor()\n\u001B[32m     12\u001B[39m     ])\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\emotions\\.venv\\Lib\\site-packages\\torch\\serialization.py:1521\u001B[39m, in \u001B[36mload\u001B[39m\u001B[34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001B[39m\n\u001B[32m   1519\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m weights_only:\n\u001B[32m   1520\u001B[39m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1521\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_load\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1522\u001B[39m \u001B[43m            \u001B[49m\u001B[43mopened_zipfile\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1523\u001B[39m \u001B[43m            \u001B[49m\u001B[43mmap_location\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1524\u001B[39m \u001B[43m            \u001B[49m\u001B[43m_weights_only_unpickler\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1525\u001B[39m \u001B[43m            \u001B[49m\u001B[43moverall_storage\u001B[49m\u001B[43m=\u001B[49m\u001B[43moverall_storage\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1526\u001B[39m \u001B[43m            \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mpickle_load_args\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1527\u001B[39m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1528\u001B[39m     \u001B[38;5;28;01mexcept\u001B[39;00m pickle.UnpicklingError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m   1529\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m pickle.UnpicklingError(_get_wo_message(\u001B[38;5;28mstr\u001B[39m(e))) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\emotions\\.venv\\Lib\\site-packages\\torch\\serialization.py:2113\u001B[39m, in \u001B[36m_load\u001B[39m\u001B[34m(zip_file, map_location, pickle_module, pickle_file, overall_storage, **pickle_load_args)\u001B[39m\n\u001B[32m   2110\u001B[39m \u001B[38;5;66;03m# Load the data (which may in turn use `persistent_load` to load tensors)\u001B[39;00m\n\u001B[32m   2111\u001B[39m data_file = io.BytesIO(zip_file.get_record(pickle_file))\n\u001B[32m-> \u001B[39m\u001B[32m2113\u001B[39m unpickler = \u001B[43mUnpicklerWrapper\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata_file\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mpickle_load_args\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   2114\u001B[39m unpickler.persistent_load = persistent_load\n\u001B[32m   2115\u001B[39m \u001B[38;5;66;03m# Needed for tensors where storage device and rebuild tensor device are\u001B[39;00m\n\u001B[32m   2116\u001B[39m \u001B[38;5;66;03m# not connected (wrapper subclasses and tensors rebuilt using numpy)\u001B[39;00m\n",
      "\u001B[31mTypeError\u001B[39m: Unpickler.__init__() got an unexpected keyword argument 'warn'"
     ]
    }
   ],
   "execution_count": 50
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HrJSxz_O6D43"
   },
   "source": [
    "## Задание 3.\n",
    "\n",
    "Выполнить задание 2, используя предобученную сеть AlexNet, доступную в PyTorch ([ссылка](https://pytorch.org/hub/pytorch_vision_alexnet/)). Выполнять дообучение сети не нужно."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "5YqxfC726D43"
   },
   "source": [
    "# Ваш код!"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a02c7c5318cacc33003ebac066976acb71fb3de0fa39ce8113ef0cea6e2a0c27"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "name": "python3",
   "language": "python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "colab": {
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
